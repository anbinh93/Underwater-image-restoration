# âš ï¸ URGENT: OOM Fix - GPU háº¿t memory khi train

## TÃ¬nh tráº¡ng hiá»‡n táº¡i
- **Lá»—i**: `torch.OutOfMemoryError: Tried to allocate 24.00 GiB`
- **GPU**: 39.49GB total, nhÆ°ng cáº§n allocate 24GB cho má»™t attention matrix
- **NguyÃªn nhÃ¢n**: Attention matrix quÃ¡ lá»›n vá»›i image size lá»›n

## ğŸ”¥ Giáº£i phÃ¡p kháº©n cáº¥p (Chá»n 1 trong 3)

### Giáº£i phÃ¡p 1: Giáº£m image size (Nhanh nháº¥t - 30 giÃ¢y)
```bash
# TrÃªn server, cháº¡y ngay:
cd /home/ec2-user/SageMaker/Underwater-image-restoration

# Train vá»›i image size nhá» hÆ¡n
bash train_ddp.sh 4 2 20 96 64
#                  â”‚ â”‚ â”‚  â”‚  â””â”€ attn_chunk=64 (memory-efficient)
#                  â”‚ â”‚ â”‚  â””â”€â”€â”€â”€ img_size=96 (thay vÃ¬ 128)
#                  â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€ epochs=20
#                  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€ batch=2/GPU
#                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4 GPUs
```

### Giáº£i phÃ¡p 2: Git pull + retry (Khuyáº¿n nghá»‹ - 2 phÃºt)
```bash
cd /home/ec2-user/SageMaker/Underwater-image-restoration
git pull origin main

# Verify chunked attention Ä‘Æ°á»£c update
grep -n "attn_chunk_size" underwater_ir/student/naf_unet_wfi/wtb.py
# Expected: Pháº£i tháº¥y self.attn_chunk_size trong code

# Retry vá»›i config an toÃ n
bash train_ddp.sh 4 2 20 128 128
```

### Giáº£i phÃ¡p 3: Giáº£m batch size + chunk size (An toÃ n nháº¥t)
```bash
# Ultra conservative settings cho GPU 40GB
bash train_ddp.sh 4 1 20 128 64
#                  â”‚ â”‚      â”‚  â””â”€ chunk=64 (4x memory efficient)
#                  â”‚ â”‚      â””â”€â”€â”€â”€ img=128
#                  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ batch=1/GPU (4 total)
#                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4 GPUs
```

## ğŸ“Š Memory Breakdown

### Vá»›i IMG_SIZE=128, BATCH=2
```
Input: [2, 3, 128, 128]
After stem: [2, 64, 128, 128]
After downsample 1: [2, 128, 64, 64]   â† 4096 tokens
After downsample 2: [2, 256, 32, 32]   â† 1024 tokens

Attention matrix cho 1024 tokens:
- Size: [batch, heads, tokens, tokens] = [2, 12, 1024, 1024]
- Memory: 2 Ã— 12 Ã— 1024Â² Ã— 4 bytes = 100MB âœ…

NhÆ°ng cÃ³ nhiá»u WTB blocks â†’ tá»•ng memory cao!
```

### Attention Chunk Size Impact
| Chunk Size | Max Attention Memory | Speed | Recommendation |
|------------|---------------------|-------|----------------|
| 1024 (full)| 100MB per block     | Fast  | âŒ OOM with big images |
| 256        | 25MB per block      | 90%   | âœ… Default |
| 128        | 12.5MB per block    | 80%   | âœ… Safe for 16GB GPU |
| 64         | 6.25MB per block    | 70%   | âœ… Ultra safe |

## ğŸ¯ Recommended Settings by GPU

### GPU 40GB (A100) - Báº¡n Ä‘ang dÃ¹ng
```bash
# Option A: Max performance
bash train_ddp.sh 4 4 20 128 256

# Option B: Balanced
bash train_ddp.sh 4 2 20 160 128

# Option C: Safe (current issue)
bash train_ddp.sh 4 2 20 128 128
```

### GPU 24GB (RTX 3090/4090)
```bash
bash train_ddp.sh 4 2 20 128 128
```

### GPU 16GB (V100)
```bash
bash train_ddp.sh 4 2 20 96 64
```

## ğŸ” Debug: Kiá»ƒm tra server cÃ³ code má»›i khÃ´ng

```bash
# Check wtb.py cÃ³ chunked attention chÆ°a
cd /home/ec2-user/SageMaker/Underwater-image-restoration
head -50 underwater_ir/student/naf_unet_wfi/wtb.py | grep -A 5 "def __init__"

# Expected output pháº£i cÃ³:
# def __init__(self, ..., attn_chunk_size: int = 256):
#     self.attn_chunk_size = attn_chunk_size

# Check forward cÃ³ dÃ¹ng chunk khÃ´ng
grep -A 20 "def forward" underwater_ir/student/naf_unet_wfi/wtb.py | grep chunk

# Expected: pháº£i tháº¥y "for i in range(0, h * w, self.attn_chunk_size)"
```

## âš¡ Immediate Action

**Cháº¡y ngay lá»‡nh nÃ y Ä‘á»ƒ test:**
```bash
cd /home/ec2-user/SageMaker/Underwater-image-restoration

# Kill training Ä‘ang cháº¡y (náº¿u cÃ³)
pkill -f "torchrun"

# Git pull Ä‘á»ƒ láº¥y code má»›i
git stash  # Save local changes if any
git pull origin main

# Verify update
python3 -c "
from underwater_ir.student.naf_unet_wfi.wtb import WideTransformerBlock
import inspect
sig = inspect.signature(WideTransformerBlock.__init__)
print('âœ… Updated!' if 'attn_chunk_size' in sig.parameters else 'âŒ Old version!')
"

# Start training vá»›i safe config
bash train_ddp.sh 4 2 20 128 128
```

## ğŸ“ˆ Monitor Memory Usage

### Terminal 1: Training
```bash
bash train_ddp.sh 4 2 20 128 128
```

### Terminal 2: GPU Monitor
```bash
watch -n 1 nvidia-smi --query-gpu=index,memory.used,memory.total,utilization.gpu --format=csv
```

### Terminal 3: Process Memory
```bash
watch -n 2 "ps aux | grep python | grep -v grep | awk '{print \$2, \$4, \$11}' | head -8"
```

## ğŸš¨ Náº¿u váº«n OOM sau khi git pull

### Reduce everything:
```bash
bash train_ddp.sh 4 1 20 96 64
```

### Hoáº·c train single GPU trÆ°á»›c:
```bash
python -m underwater_ir.student.train_student \
  --train-root Dataset/train \
  --pseudo-root pseudo-labels/daclip \
  --epochs 20 \
  --batch-size 4 \
  --img-size 128 \
  --attn-chunk-size 128 \
  --save-path experiments/student_single_gpu.pt
```

## ğŸ’¡ Why Still OOM?

1. **Server chÆ°a cÃ³ code má»›i**: Git pull Ä‘á»ƒ update
2. **Python cache**: `rm -rf __pycache__` vÃ  `rm -rf underwater_ir/**/__pycache__`
3. **Multiple forward passes**: Gradient accumulation khÃ´ng Ä‘Æ°á»£c implement
4. **Memory fragmentation**: Set `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True`

## ğŸ”§ Advanced: Set environment variables

```bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export CUDA_LAUNCH_BLOCKING=1  # For debugging only, slower

bash train_ddp.sh 4 2 20 128 128
```

## âœ… Success Criteria

Training should show:
```
âœ… Model initialized with attn_chunk_size=128 (lower=less memory)
âœ… DDP enabled: training on 4 GPUs
Epoch 1/20 - Train loss: 0.xxxx
```

GPU memory should be:
```
| GPU | Memory-Usage |
|  0  | 15GB/40GB    |  â† Should be < 30GB
|  1  | 15GB/40GB    |
|  2  | 15GB/40GB    |
|  3  | 15GB/40GB    |
```
